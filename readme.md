# Proyecto: Predicci√≥n de Precios de Propiedades en Argentina

**Proyecto integrador ‚Äî 5HP** **Curso:** Programaci√≥n Avanzada para Ciencia de Datos  
**Universidad:** Universidad de la ciudad de Buenos Aires  
**Equipo 5HP:**
- MATIAS ALEJANDRO BANCHIO
- PABLO GABRIEL CIOCIANO
- PAULA GISELA COCHIMANO
- ANTONIO LUIS EMILIO MARTINEZ
- ENRIQUE IGNACIO VAZQUEZ

---

## üöÄ Resumen del Proyecto

Este proyecto analiza un conjunto de datos de propiedades en Argentina (dataset de Properati) con el objetivo de desarrollar un modelo de *machine learning* capaz de predecir el precio de venta (en USD) de un inmueble en funci√≥n de sus caracter√≠sticas principales, como la ubicaci√≥n, el tipo de propiedad, la superficie, y la cantidad de ambientes y ba√±os.

El pipeline completo incluye la limpieza y preprocesamiento de datos, un an√°lisis exploratorio (EDA), entrenamiento y comparaci√≥n de m√∫ltiples modelos de regresi√≥n, y una optimizaci√≥n final mediante ajuste de hiperpar√°metros.

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

* **Lenguaje:** Python 3.10+
* **An√°lisis y Manipulaci√≥n de Datos:** Pandas, NumPy
* **Bases de Datos:** DuckDB (para persistencia de resultados anal√≠ticos)
* **Visualizaci√≥n:** Matplotlib, Seaborn
* **Machine Learning:** Scikit-learn (para pipelines, preprocesamiento, `train_test_split`, `GridSearchCV`, `LinearRegression` y `RandomForestRegressor`)
* **Modelado Avanzado:** XGBoost (para `XGBRegressor`)
* **Serializaci√≥n de Modelos:** Joblib
* **Entorno:** Jupyter Notebook / Google Colab

---

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n

1.  **Clonar el repositorio:**
    ```bash
    git clone [URL-DEL-REPOSITORIO]
    cd [NOMBRE-DEL-REPOSITORIO]
    ```

2.  **Crear un entorno virtual (recomendado):**
    ```bash
    python -m venv env
    source env/bin/activate  # En Windows: env\Scripts\activate
    ```

3.  **Instalar las dependencias:**
    Se puede crear un archivo `requirements.txt` con el siguiente contenido e instalarlo.

    **requirements.txt:**
    ```
    pandas
    numpy
    duckdb
    matplotlib
    seaborn
    joblib
    scikit-learn
    xgboost
    jupyter
    ```

    **Comando de instalaci√≥n:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Descargar los datos:**
    Aseg√∫rese de tener el archivo `entrenamiento.csv` ubicado en la carpeta `data/`.

---

## ‚ñ∂Ô∏è C√≥mo Ejecutar el Pipeline

1.  Inicie Jupyter Notebook en su terminal:
    ```bash
    jupyter notebook
    ```
2.  Abra el archivo `TPFinal_.ipynb`.
3.  Ejecute todas las celdas en orden, desde la importaci√≥n de librer√≠as hasta la persistencia de datos. Los artefactos generados (dataset limpio, modelo y base de datos) se guardar√°n en la carpeta `data/`.

---

## üìñ Descripci√≥n del Notebook (`TPFinal_.ipynb`)

El notebook est√° estructurado en 7 secciones principales que siguen un flujo de trabajo est√°ndar de ciencia de datos.

### 1. Carga y Limpieza de Datos üßπ
* Se carga el dataset crudo `entrenamiento.csv`.
* Se aplica un filtro inicial para mantener solo operaciones de **Venta** y en moneda **USD**.
* Se seleccionan las columnas clave: `price`, `surface_covered`, `rooms`, `bathrooms`, `property_type`, y `l2` (ciudad).
* Se eliminan todos los registros con valores nulos en estas columnas.
* Se filtran valores at√≠picos (outliers) de precio, manteniendo solo propiedades entre $10,000 y $1,000,000 USD para estabilizar el modelo.
* Se renombran las columnas para mayor claridad (ej. `l2` a `city`).
* El dataset limpio resultante se guarda como `data/cleaned_data.csv`.

### 2. An√°lisis Exploratorio de Datos (EDA) üìä
* Se analiza la distribuci√≥n de la variable objetivo (`price`) mediante un histograma, mostrando un sesgo a la derecha.
* Se genera una matriz de correlaci√≥n (heatmap) para las variables num√©ricas (`price`, `surface`, `rooms`, `bathrooms`), identificando una fuerte correlaci√≥n positiva entre el precio y la superficie.
* Se utiliza un diagrama de caja (boxplot) para visualizar la distribuci√≥n de precios seg√∫n el `property_type`.

### 3. Ingenier√≠a de Features y Divisi√≥n de Datos üî™
* Se definen las variables predictoras (X) y la variable objetivo (y).
* Se realiza una divisi√≥n de los datos en conjuntos de entrenamiento (80%) y prueba (20%) usando `train_test_split`, asegurando la reproducibilidad con `random_state=42`.

### 4. Creaci√≥n del Pipeline y Modelos Base ü§ñ
* Se define un `ColumnTransformer` para el preprocesamiento autom√°tico:
    * **Variables Num√©ricas** (`surface`, `rooms`, `bathrooms`): Se escalan con `StandardScaler`.
    * **Variables Categ√≥ricas** (`property_type`, `city`): Se codifican con `OneHotEncoder`.
* Se crea un `Pipeline` de Scikit-learn que integra el preprocesador y el modelo.
* Se entrenan y eval√∫an tres modelos base para comparar rendimiento:
    1.  `LinearRegression`
    2.  `RandomForestRegressor`
    3.  `XGBRegressor`
* Las m√©tricas de evaluaci√≥n (MAE, RMSE, R¬≤) se almacenan en un DataFrame (`results_df`).

### 5. Ajuste de Hiperpar√°metros (Hyperparameter Tuning) üõ†Ô∏è
* Se selecciona el modelo con mejor rendimiento base (en este caso, `XGBoost`) para una optimizaci√≥n m√°s profunda.
* Se utiliza `GridSearchCV` para encontrar la mejor combinaci√≥n de hiperpar√°metros (ej. `n_estimators`, `max_depth`, `min_samples_leaf`) para el `RandomForestRegressor` (como alternativa robusta).
* Se identifica y almacena el `best_model` (el pipeline optimizado).

### 6. Evaluaci√≥n del Modelo Final y An√°lisis üìà
* Se eval√∫a el modelo optimizado (`best_model`) contra el conjunto de prueba.
* Se generan gr√°ficos comparativos (barplots) de **RMSE** y **R¬≤** para todos los modelos (base y optimizado), confirmando que XGBoost ofrece el mejor rendimiento (R¬≤ ~0.705).
* Se crea un gr√°fico de dispersi√≥n (scatterplot) de **Valores Reales vs. Valores Predichos** para evaluar visualmente la precisi√≥n y el sesgo del modelo final.
* Se genera un gr√°fico de **Importancia de Features** (del modelo Random Forest) para entender qu√© variables contribuyen m√°s a la predicci√≥n del precio.

### 7. Persistencia del Modelo y Resultados üíæ
* El pipeline completo del mejor modelo (incluyendo preprocesador y modelo entrenado) se serializa y guarda en un archivo `data/best_model.pkl` usando `joblib`.
* Los resultados clave del an√°lisis se guardan en una base de datos **DuckDB** (`data/properati_models.db`) en tres tablas separadas para consulta futura:
    * `input_data`: El DataFrame limpio usado para el an√°lisis.
    * `model_results`: El DataFrame con las m√©tricas de todos los modelos.
    * `model_config`: Los mejores hiperpar√°metros encontrados por `GridSearchCV`.